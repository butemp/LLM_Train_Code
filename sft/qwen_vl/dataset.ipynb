{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/new_disk/czy/anaconda3/envs/llavanext/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Tuple\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class text_image:\n",
    "    q_input_ids : torch.Tensor\n",
    "    pixel_values : torch.Tensor\n",
    "    a_input_ids : torch.Tensor\n",
    "    image_grid_thw : torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlavaDataset(Dataset):\n",
    "    def __init__(self, data_file, image_dir):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_file = data_file\n",
    "        self.image_dir = image_dir\n",
    "        self.chat_data = pd.read_json(data_file).to_dict(orient=\"records\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chat_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.chat_data[index]\n",
    "        messages = data['messages']\n",
    "        human_input = messages[0]['content']\n",
    "        gpt_output = messages[1]['content']\n",
    "        images = data['images'] # List\n",
    "        images = [os.path.join(self.image_dir,image) for image in images]\n",
    "        return human_input, gpt_output, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = LlavaDataset('/mnt/new_disk/qjw/my_llm_code/sft/mllm_demo.json','/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<image>Who are they?',\n",
       " \"They're Kane and Gretzka from Bayern Munich.\",\n",
       " ['/mllm_demo_data/1.jpg', '/mllm_demo_data/1.jpg'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained('/mnt/new_disk/qjw/ckpt/qwen/Qwen2-VL-7B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [27, 1805, 29], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer('<image>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_images = Image.open('/mnt/new_disk/qjw/assert/498339.jpg')\n",
    "text = 'this is a image'\n",
    "text = [\n",
    "    {\"role\":\"user\",\"content\":text},\n",
    "]\n",
    "text = processor.apply_chat_template(\n",
    "        text, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "inputs = processor(text=text, images=[raw_images,raw_images], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7992, 1176])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [574, 374, 264, 2168], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer('this is a image',padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_qa_and_image(qa_and_image : Tuple[str,str,List[str]], processor : AutoProcessor):\n",
    "#     # 需要直接传一个组数据\n",
    "    \n",
    "#     human_input, gpt_output, images = qa_and_image\n",
    "\n",
    "#     message = [\n",
    "#         {\"role\":\"user\",\"content\":human_input}\n",
    "#     ]\n",
    "\n",
    "#     # 再加模板之前，需要转为标准格式，详细见modelscope官网，以便于组batch\n",
    "#     prompt = processor.apply_chat_template(\n",
    "#         message, tokenize=False, add_generation_prompt=True\n",
    "#     )\n",
    "    \n",
    "#     # 官方处理吗\n",
    "#     raw_images = [Image.open(image) for image in images]\n",
    "\n",
    "#     inputs = processor(text=prompt, images=raw_images, return_tensors=\"pt\")  # .to(0, torch.float16)\n",
    "#     # 不要将label也一起塞进去，而是单独处理，以方便后续的拼接\n",
    "#     # inputs = dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw'])\n",
    "#     # input_ids.shape = torch.Size([1, 23])\n",
    "\n",
    "#     a_input_ids = processor.tokenizer(\n",
    "#         gpt_output,\n",
    "#         return_tensors='pt',\n",
    "#         padding=\"longest\",\n",
    "#         truncation=True,\n",
    "#     )['input_ids']\n",
    "\n",
    "#     return text_image(\n",
    "#         q_input_ids=inputs['input_ids'],\n",
    "#         pixel_values=inputs['pixel_values'],\n",
    "#         a_input_ids=a_input_ids,\n",
    "#         image_grid_thw=inputs['image_grid_thw']\n",
    "#     )\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_qa_and_image(qa_and_image : List[Tuple[str,str,List[str]]], processor : AutoProcessor):\n",
    "    # 需要直接传一个组数据，并组成官方可以支持的数据格式\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    gpt_output_list = []\n",
    "    for one_piece_data in qa_and_image:\n",
    "        human_input, gpt_output, images = one_piece_data\n",
    "        gpt_output_list.append(gpt_output)\n",
    "\n",
    "        ins = human_input.split('<image>')\n",
    "        content = []\n",
    "        for idx in range(len(ins)):\n",
    "            s = ins[idx]\n",
    "\n",
    "            if s != '':\n",
    "                content.append({\n",
    "                    \"type\":\"text\",\n",
    "                    \"text\":s\n",
    "                })\n",
    "\n",
    "            if idx != len(ins) - 1:\n",
    "                content.append({\n",
    "                    \"type\":\"image\",\n",
    "                    \"image\":images[idx]\n",
    "                })\n",
    "        message = {\n",
    "            \"role\":\"user\",\n",
    "            \"content\":content,\n",
    "        }\n",
    "        # 注意每一条数据都是一个列表，不是字典\n",
    "        messages.append([message])\n",
    "    \n",
    "\n",
    "    # ====================官方模板处理====================     \n",
    "    # 再加模板之前，需要转为标准格式，详细见modelscope官网，以便于组batch\n",
    "    # 填充是用 '<|endoftext|>' 是left_padding\n",
    "    prompt = [\n",
    "        processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n",
    "        for msg in messages\n",
    "    ]\n",
    "\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "\n",
    "    q_inputs_list = processor(\n",
    "        text=prompt,\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=False,\n",
    "    )\n",
    "    # ====================官方模板处理====================\n",
    "    # inputs = dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw'])\n",
    "    a_input_ids = processor.tokenizer(\n",
    "        gpt_output_list,\n",
    "        truncation=True,\n",
    "    )['input_ids']\n",
    "\n",
    "\n",
    "    return q_inputs_list, a_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat使用示例\n",
    "x = torch.rand(1,10)\n",
    "y = torch.concat(\n",
    "    [x,x],dim=1\n",
    ")\n",
    "y.shape # torch.Size([1, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100., -100., -100., -100., -100.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full_like(torch.rand(1,5), fill_value=-100)\n",
    "# tensor([[-100., -100., -100., -100., -100.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], dtype=torch.int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full((0,), fill_value=-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlavaDataCollator:\n",
    "    def __init__(self,processor: AutoProcessor):\n",
    "        self.processor = processor\n",
    "        self.ignore_index = -100\n",
    "\n",
    "    def processqa_for_train(self, q_input_ids, a_input_ids):\n",
    "        # 两者形状都是[1,X],直接concat\n",
    "        # 因为是sft 最后要加上eos token\n",
    "\n",
    "        if type(q_input_ids) == list:\n",
    "            q_input_ids = torch.tensor(q_input_ids).reshape(1,-1)\n",
    "            a_input_ids = torch.tensor(a_input_ids).reshape(1,-1)\n",
    "\n",
    "        eos_token_id = torch.tensor(self.processor.tokenizer.eos_token_id).reshape(1,-1)\n",
    "\n",
    "        inputs = torch.concat(\n",
    "            [q_input_ids,a_input_ids,eos_token_id],\n",
    "            dim=1\n",
    "        )\n",
    "        # sft格式的instruction对应的labels中的值设置为-100\n",
    "        # 最后的eos token也要计算loss\n",
    "        labels = torch.concat(\n",
    "            [torch.full_like(q_input_ids, fill_value=self.ignore_index),a_input_ids,eos_token_id],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "\n",
    "    \n",
    "    def __call__(self, features: List) -> Dict[str, torch.Tensor]:\n",
    "        input_ids_list = []\n",
    "        label_ids_list = []\n",
    "        pixel_values_list = []\n",
    "        max_len_list = []\n",
    "        image_grid_thw_list = []\n",
    "\n",
    "        q_inputs_list, a_input_ids = process_qa_and_image(features,self.processor)\n",
    "\n",
    "        # for feature in features:\n",
    "        #     input_ids ,labels = self.processqa_for_train(feature.q_input_ids, feature.a_input_ids)\n",
    "        #     input_ids_list.append(input_ids)\n",
    "        #     label_ids_list.append(labels)\n",
    "        #     pixel_values_list.append(feature.pixel_values)\n",
    "        #     image_grid_thw_list.append(feature.image_grid_thw)\n",
    "        #     max_len_list.append(input_ids.shape[1])\n",
    "        \n",
    "        # 要将这些数据组成一个batch，这里主要是做了padding\n",
    "        # 实际sft时还要做截断\n",
    "\n",
    "\n",
    "        # 将q和a组成训练所需要的格式\n",
    "        max_len = 0\n",
    "        for q,a in zip(q_inputs_list['input_ids'], a_input_ids):\n",
    "            input_ids ,labels = self.processqa_for_train(q, a)\n",
    "            input_ids_list.append(input_ids)\n",
    "            label_ids_list.append(labels)\n",
    "            max_len = max(max_len, input_ids.shape[1])\n",
    "\n",
    "        # pixel_values_list = torch.tensor(q_inputs_list['pixel_values'])\n",
    "        # image_grid_thw_list = torch.tensor(q_inputs_list['image_grid_thw'])\n",
    "            \n",
    "            \n",
    "        pad_token_id = self.processor.tokenizer.pad_token_id\n",
    "\n",
    "        # 这里实现的是right padding 但是训练是无所谓的\n",
    "\n",
    "        final_input_ids = torch.concat(\n",
    "            [torch.concat([input_ids,torch.full((1,max_len - input_ids.shape[1]), pad_token_id)],dim=1) for input_ids in input_ids_list],\n",
    "            dim=0\n",
    "        )\n",
    "        final_labels_ids = torch.concat(\n",
    "            [torch.concat([labels_ids,torch.full((1,max_len - labels_ids.shape[1]), pad_token_id)],dim=1) for labels_ids in label_ids_list],\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        # qwen_vl还需要个image_grid_thw参数\n",
    "        # final_image_grid_thw = torch.concat(\n",
    "        #     [image_grid_thw for image_grid_thw in image_grid_thw_list],\n",
    "        #     dim=0\n",
    "        # )\n",
    "\n",
    "        # # pixel_values不需要padding\n",
    "        # final_pixel_values = torch.concat(pixel_values_list,dim=0)\n",
    "        final_pixel_values = torch.tensor(q_inputs_list['pixel_values'])\n",
    "        final_image_grid_thw = torch.tensor(q_inputs_list['image_grid_thw'])\n",
    "\n",
    "        # attenion_mask:pad token的值应该设置为0\n",
    "        attention_mask = torch.ones_like(final_input_ids)\n",
    "        attention_mask[final_input_ids == self.processor.tokenizer.pad_token_id] = 0\n",
    "\n",
    "        return {\n",
    "            \"input_ids\":final_input_ids,\n",
    "            \"labels\":final_labels_ids,\n",
    "            \"pixel_values\":final_pixel_values,\n",
    "            'image_grid_thw':final_image_grid_thw,\n",
    "            \"attention_mask\":attention_mask\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "processor = AutoProcessor.from_pretrained('/mnt/new_disk/qjw/ckpt/Qwen/Qwen2-VL-2B-Instruct')\n",
    "dataset = LlavaDataset('/mnt/new_disk/qjw/my_llm_code/sft/train_data.json','/mnt/new_disk/qjw/LLaMA-Factory/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset,2,collate_fn=LlavaDataCollator(processor=processor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3796])\n",
      "tensor([[  1, 172,  78],\n",
      "        [  1, 172,  78]])\n",
      "torch.Size([2, 4013])\n",
      "tensor([[  1, 170,  82],\n",
      "        [  1, 172,  78]])\n",
      "torch.Size([2, 5117])\n",
      "tensor([[  1, 198,  92],\n",
      "        [  1, 148,  84]])\n",
      "torch.Size([2, 4373])\n",
      "tensor([[  1, 180,  84],\n",
      "        [  1, 164,  92]])\n",
      "torch.Size([2, 4689])\n",
      "tensor([[  1, 146, 110],\n",
      "        [  1,  92,  52]])\n",
      "torch.Size([2, 2942])\n",
      "tensor([[  1, 138,  78],\n",
      "        [  1,  72,  84]])\n",
      "torch.Size([2, 4120])\n",
      "tensor([[  1, 180,  84],\n",
      "        [  1, 180,  84]])\n",
      "torch.Size([2, 4428])\n",
      "tensor([[  1,  92,  52],\n",
      "        [  1, 182,  84]])\n",
      "torch.Size([2, 4544])\n",
      "tensor([[  1, 192,  88],\n",
      "        [  1, 172,  78]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_grid_thw\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/mnt/new_disk/czy/anaconda3/envs/llavanext/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/mnt/new_disk/czy/anaconda3/envs/llavanext/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/mnt/new_disk/czy/anaconda3/envs/llavanext/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[71], line 38\u001b[0m, in \u001b[0;36mLlavaDataCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     35\u001b[0m max_len_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     36\u001b[0m image_grid_thw_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 38\u001b[0m q_inputs_list, a_input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_qa_and_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# for feature in features:\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#     input_ids ,labels = self.processqa_for_train(feature.q_input_ids, feature.a_input_ids)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#     input_ids_list.append(input_ids)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# 将q和a组成训练所需要的格式\u001b[39;00m\n\u001b[1;32m     53\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[67], line 46\u001b[0m, in \u001b[0;36mprocess_qa_and_image\u001b[0;34m(qa_and_image, processor)\u001b[0m\n\u001b[1;32m     38\u001b[0m prompt \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     39\u001b[0m     processor\u001b[38;5;241m.\u001b[39mapply_chat_template(msg, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[1;32m     41\u001b[0m ]\n\u001b[1;32m     43\u001b[0m image_inputs, video_inputs \u001b[38;5;241m=\u001b[39m process_vision_info(messages)\n\u001b[0;32m---> 46\u001b[0m q_inputs_list \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# ====================官方模板处理====================\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# inputs = dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw'])\u001b[39;00m\n\u001b[1;32m     54\u001b[0m a_input_ids \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m     55\u001b[0m     gpt_output_list,\n\u001b[1;32m     56\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     57\u001b[0m )[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/mnt/new_disk/czy/anaconda3/envs/llavanext/lib/python3.10/site-packages/transformers/models/qwen2_vl/processing_qwen2_vl.py:115\u001b[0m, in \u001b[0;36mQwen2VLProcessor.__call__\u001b[0;34m(self, images, text, videos, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m output_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_kwargs(\n\u001b[1;32m    110\u001b[0m     Qwen2VLProcessorKwargs,\n\u001b[1;32m    111\u001b[0m     tokenizer_init_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39minit_kwargs,\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    113\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     image_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutput_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages_kwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     image_grid_thw \u001b[38;5;241m=\u001b[39m image_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_grid_thw\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/new_disk/czy/anaconda3/envs/llavanext/lib/python3.10/site-packages/transformers/image_processing_utils.py:41\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/new_disk/czy/anaconda3/envs/llavanext/lib/python3.10/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl.py:417\u001b[0m, in \u001b[0;36mQwen2VLImageProcessor.preprocess\u001b[0;34m(self, images, videos, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    415\u001b[0m pixel_values, vision_grid_thws \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[0;32m--> 417\u001b[0m     patches, image_grid_thw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_resize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_resize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_rescale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrescale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrescale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_normalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_convert_rgb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_convert_rgb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     pixel_values\u001b[38;5;241m.\u001b[39mextend(patches)\n\u001b[1;32m    431\u001b[0m     vision_grid_thws\u001b[38;5;241m.\u001b[39mappend(image_grid_thw)\n",
      "File \u001b[0;32m/mnt/new_disk/czy/anaconda3/envs/llavanext/lib/python3.10/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl.py:291\u001b[0m, in \u001b[0;36mQwen2VLImageProcessor._preprocess\u001b[0;34m(self, images, do_resize, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    288\u001b[0m     image \u001b[38;5;241m=\u001b[39m to_channel_dimension_format(image, data_format, input_channel_dim\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    289\u001b[0m     processed_images\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[0;32m--> 291\u001b[0m patches \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_format \u001b[38;5;241m==\u001b[39m ChannelDimension\u001b[38;5;241m.\u001b[39mLAST:\n\u001b[1;32m    293\u001b[0m     patches \u001b[38;5;241m=\u001b[39m patches\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch in data_loader:\n",
    "    print(batch['input_ids'].shape)\n",
    "    print(batch['image_grid_thw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
